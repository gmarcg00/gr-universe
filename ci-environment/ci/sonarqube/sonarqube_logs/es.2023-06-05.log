2023.06.05 10:49:25 INFO  es[][o.e.n.Node] version[7.17.8], pid[27], build[default/tar/120eabe1c8a0cb2ae87cffc109a5b65d213e9df1/2022-12-02T17:33:09.727072865Z], OS[Linux/5.15.90.1-microsoft-standard-WSL2/amd64], JVM[Eclipse Adoptium/OpenJDK 64-Bit Server VM/17.0.7/17.0.7+7]
2023.06.05 10:49:25 INFO  es[][o.e.n.Node] JVM home [/opt/java/openjdk]
2023.06.05 10:49:25 INFO  es[][o.e.n.Node] JVM arguments [-XX:+UseG1GC, -Djava.io.tmpdir=/opt/sonarqube/temp, -XX:ErrorFile=/opt/sonarqube/logs/es_hs_err_pid%p.log, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Djna.tmpdir=/opt/sonarqube/temp, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=COMPAT, -Dcom.redhat.fips=false, -Xmx512m, -Xms512m, -XX:MaxDirectMemorySize=256m, -XX:+HeapDumpOnOutOfMemoryError, -Des.path.home=/opt/sonarqube/elasticsearch, -Des.path.conf=/opt/sonarqube/temp/conf/es, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=false]
2023.06.05 10:49:26 INFO  es[][o.e.p.PluginsService] loaded module [analysis-common]
2023.06.05 10:49:26 INFO  es[][o.e.p.PluginsService] loaded module [lang-painless]
2023.06.05 10:49:26 INFO  es[][o.e.p.PluginsService] loaded module [parent-join]
2023.06.05 10:49:26 INFO  es[][o.e.p.PluginsService] loaded module [reindex]
2023.06.05 10:49:26 INFO  es[][o.e.p.PluginsService] loaded module [transport-netty4]
2023.06.05 10:49:26 INFO  es[][o.e.p.PluginsService] no plugins loaded
2023.06.05 10:49:26 INFO  es[][o.e.e.NodeEnvironment] using [1] data paths, mounts [[/opt/sonarqube/data (drvfs)]], net usable_space [268.3gb], net total_space [465.1gb], types [9p]
2023.06.05 10:49:26 INFO  es[][o.e.e.NodeEnvironment] heap size [512mb], compressed ordinary object pointers [true]
2023.06.05 10:49:30 INFO  es[][o.e.n.Node] node name [sonarqube], node ID [-lj9NGM-QtiZkza0N3P-2g], cluster name [sonarqube], roles [data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
2023.06.05 10:49:35 INFO  es[][o.e.t.NettyAllocator] creating NettyAllocator with the following configs: [name=unpooled, suggested_max_allocation_size=256kb, factors={es.unsafe.use_unpooled_allocator=null, g1gc_enabled=true, g1gc_region_size=1mb, heap_size=512mb}]
2023.06.05 10:49:35 INFO  es[][o.e.i.r.RecoverySettings] using rate limit [40mb] with [default=40mb, read=0b, write=0b, max=0b]
2023.06.05 10:49:35 INFO  es[][o.e.d.DiscoveryModule] using discovery type [zen] and seed hosts providers [settings]
2023.06.05 10:49:35 INFO  es[][o.e.g.DanglingIndicesState] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
2023.06.05 10:49:35 INFO  es[][o.e.n.Node] initialized
2023.06.05 10:49:35 INFO  es[][o.e.n.Node] starting ...
2023.06.05 10:49:36 INFO  es[][o.e.t.TransportService] publish_address {127.0.0.1:37405}, bound_addresses {127.0.0.1:37405}
2023.06.05 10:49:37 WARN  es[][o.e.b.BootstrapChecks] max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
2023.06.05 10:49:37 INFO  es[][o.e.c.c.Coordinator] cluster UUID [xGLWHFVjSbimc3Jxm-eWyw]
2023.06.05 10:49:37 INFO  es[][o.e.c.s.MasterService] elected-as-master ([1] nodes joined)[{sonarqube}{-lj9NGM-QtiZkza0N3P-2g}{h_bdBqKRRBaX6kIDB9yiwg}{127.0.0.1}{127.0.0.1:37405}{cdfhimrsw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 32, version: 853, delta: master node changed {previous [], current [{sonarqube}{-lj9NGM-QtiZkza0N3P-2g}{h_bdBqKRRBaX6kIDB9yiwg}{127.0.0.1}{127.0.0.1:37405}{cdfhimrsw}]}
2023.06.05 10:49:38 INFO  es[][o.e.c.s.ClusterApplierService] master node changed {previous [], current [{sonarqube}{-lj9NGM-QtiZkza0N3P-2g}{h_bdBqKRRBaX6kIDB9yiwg}{127.0.0.1}{127.0.0.1:37405}{cdfhimrsw}]}, term: 32, version: 853, reason: Publication{term=32, version=853}
2023.06.05 10:49:38 INFO  es[][o.e.h.AbstractHttpServerTransport] publish_address {127.0.0.1:9001}, bound_addresses {127.0.0.1:9001}
2023.06.05 10:49:38 INFO  es[][o.e.n.Node] started
2023.06.05 10:49:38 INFO  es[][o.e.g.GatewayService] recovered [7] indices into cluster_state
2023.06.05 10:50:01 INFO  es[][o.e.c.r.a.AllocationService] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[metadatas][0]]]).
2023.06.05 12:17:35 WARN  es[][o.e.h.AbstractHttpServerTransport] handling request [null][GET][/_cluster/health?master_timeout=30s&level=cluster&timeout=30s][Netty4HttpChannel{localAddress=/127.0.0.1:9001, remoteAddress=/127.0.0.1:59722}] took [5697ms] which is above the warn threshold of [5000ms]
2023.06.05 12:17:35 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.6s/5698ms] on absolute clock which is above the warn threshold of [5000ms]
2023.06.05 12:17:36 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.6s/5697938375ns] on relative clock which is above the warn threshold of [5000ms]
2023.06.05 13:34:03 WARN  es[][o.e.t.ThreadPool] timer thread slept for [1.2h/4584049ms] on absolute clock which is above the warn threshold of [5000ms]
2023.06.05 13:36:58 WARN  es[][o.e.t.ThreadPool] timer thread slept for [7s/7087ms] on absolute clock which is above the warn threshold of [5000ms]
2023.06.05 17:58:15 WARN  es[][o.e.t.ThreadPool] timer thread slept for [1.5h/5715697ms] on absolute clock which is above the warn threshold of [5000ms]
2023.06.05 19:45:55 WARN  es[][o.e.t.ThreadPool] timer thread slept for [7.4s/7414ms] on absolute clock which is above the warn threshold of [5000ms]
2023.06.05 19:45:55 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@ac6faea, interval=1s}] took [7413ms] which is above the warn threshold of [5000ms]
2023.06.05 22:47:29 WARN  es[][o.e.t.ThreadPool] timer thread slept for [7.4s/7413425075ns] on relative clock which is above the warn threshold of [5000ms]
2023.06.05 22:47:29 WARN  es[][o.e.t.ThreadPool] timer thread slept for [3h/10894213ms] on absolute clock which is above the warn threshold of [5000ms]
2023.06.05 23:28:57 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@24642eba, interval=5s}] took [13591ms] which is above the warn threshold of [5000ms]
2023.06.05 23:28:57 WARN  es[][o.e.t.ThreadPool] timer thread slept for [13.5s/13591ms] on absolute clock which is above the warn threshold of [5000ms]
2023.06.05 23:28:57 WARN  es[][o.e.t.ThreadPool] timer thread slept for [13.5s/13591564326ns] on relative clock which is above the warn threshold of [5000ms]
